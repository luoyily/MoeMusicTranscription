{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from hppnet.transcriber import HPPNet\n",
    "from hppnet.midi import save_midi\n",
    "from hppnet.decoding import extract_notes\n",
    "import numpy as np\n",
    "from mir_eval.util import midi_to_hz\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "HOP_LENGTH = SAMPLE_RATE * 20 // 1000\n",
    "MIN_MIDI = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'checkpoints/model-142000-mestro-maps_mus-my-f0.912n0.973.pt'\n",
    "device = 'cuda'\n",
    "model = torch.load(model_path, map_location=device).eval()\n",
    "model.inference_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = '恋×シンアイ彼女/水月陵 - flower -piano arrangement-.flac'\n",
    "audio, sr = librosa.load(audio_path,sr=SAMPLE_RATE,mono=True)\n",
    "\n",
    "audio = torch.tensor(audio)\n",
    "audio_length = len(audio)\n",
    "audio = audio.reshape(-1, audio.shape[-1])\n",
    "\n",
    "onset_threshold = 0.5\n",
    "frame_threshold = 0.4\n",
    "clip_len = 4096\n",
    "n_step = (audio_length - 1) // HOP_LENGTH + 1\n",
    "\n",
    "if n_step <= clip_len:\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        pred = model.forward(audio)\n",
    "# clip audio to fixed length to prevent out of memory.\n",
    "else:  # when test on long audio\n",
    "    print('n_step > clip_len %d ' % clip_len, audio.shape)\n",
    "    clip_list = [clip_len] * (n_step // clip_len)\n",
    "    res = n_step % clip_len\n",
    "    # clip_list.append(res)\n",
    "    if (n_step > clip_len and res != 0):\n",
    "        clip_list[-1] -= (clip_len - res)//2\n",
    "        clip_list += [res + (clip_len - res)//2]\n",
    "\n",
    "    print('clip list:', clip_list)\n",
    "\n",
    "    begin = 0\n",
    "    pred = {}\n",
    "    losses = {}\n",
    "    for clip in clip_list:\n",
    "        end = begin + clip\n",
    "        audio_i = audio[0][HOP_LENGTH*begin:HOP_LENGTH*end]\n",
    "        audio_i = audio_i.unsqueeze(0)\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            pred_i = model.forward(audio_i)\n",
    "\n",
    "        for key, item in pred_i.items():\n",
    "            if (key in pred):\n",
    "                item = item[:,:,:clip,:].to('cpu')\n",
    "                pred[key] = torch.cat([pred[key].to('cpu'), item], dim=2)\n",
    "            else:\n",
    "                pred[key] = item\n",
    "        begin += clip\n",
    "for key, value in pred.items():\n",
    "    value.squeeze_(0).relu_()\n",
    "    value.squeeze_(0)\n",
    "\n",
    "\n",
    "# pitch, interval, velocity\n",
    "p_est, i_est, v_est = extract_notes(\n",
    "    pred['onset'], pred['frame'], pred['velocity'], onset_threshold, frame_threshold)\n",
    "scaling = HOP_LENGTH / SAMPLE_RATE\n",
    "i_est = (i_est * scaling).reshape(-1, 2)\n",
    "p_est = np.array([midi_to_hz(MIN_MIDI + midi) for midi in p_est])\n",
    "midi_path =audio_path.split('/')[-1]+model_path.split('/')[-1]+'.mid'\n",
    "\n",
    "# Save midi file\n",
    "save_midi(midi_path, p_est, i_est, v_est)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
